# @package _global_
training:
  num_gpus: 1
  batch_size: 2
  eval_batch_size: 1
  batch_metrics:
  lr: 0.0001
  max_epochs: ${general.max_epochs}
  min_epochs: ${general.min_epochs}
  num_workers:
  mode: 'min'            # 'min' or 'max', will minimize or maximize the chosen metric
  max_used_ram:
  max_used_perc:
  state_dict_path:
  state_dict_strict_load: True

#  precision: 16
#  step_size: 4
#  weight_decay: 0
#  weights_summary:
#  gradient_clip_val: 0.5
#  seed: 666
#  gamma: 0.9
#  profiler: off    # option are off, 'simple' or 'pytorch'
#  dropout: False   # (bool) Use dropout or not
