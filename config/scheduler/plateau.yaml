# @package _global_
scheduler:
  class_name: torch.optim.lr_scheduler.ReduceLROnPlateau
  step: epoch
  monitor: ${loss}
  params:
    mode: ${training.mode}
    factor: 0.5  # (float) Set dropout probability, e.g. 0.5
    patience: 15