# @package _global_
optimizer:
  class_name: torch.optim.AdamW
  params:
    lr: ${training.lr}
    weight_decay: 0.001
    # decoder_lr: 0.01