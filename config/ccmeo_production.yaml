defaults:
  - model: smp_unet
  - inference: production
  - postprocess: production
  - evaluate: default_evaluate
  - callbacks: default_callbacks
  - dataset: ccmeo_datacube_dataset
  - augmentation: basic_augmentation_segmentation
  - optimizer: adam
  - scheduler: plateau
  - visualization: default_visualization
  - tracker: # set logger here or use command line (e.g. `python GDL.py tracker=mlflow`)
  - hydra: default
  - loss: binary/softbce  # TODO: remove from utils/print_config()
  - training: default_training
  - override hydra/hydra_logging: colorlog # enable color logging to make it pretty
  - override hydra/job_logging: colorlog # enable color logging to make it pretty
  - _self_

general:
  # path to original working directory
  # hydra hijacks working directory by changing it to the current log directory,
  # so it's useful to have this path as a special variable
  # learn more here: https://hydra.cc/docs/next/tutorials/basic/running_your_app/working_directory
  task: segmentation
  work_dir: ${hydra:runtime.cwd}  # where the code is executed
  config_name: ${hydra:job.config_name}
  config_override_dirname: ${hydra:job.override_dirname}
  config_path: ${hydra:runtime.config_sources}
  project_name: template_project
  workspace: your_name
  raw_data_dir: data
  raw_data_csv: tests/sampling/sampling_segmentation_binary_ci.csv
  sample_data_dir: data # where the hdf5 will be saved
  state_dict_path:
  save_weights_dir: saved_model/${general.project_name}

AWS:
  bucket_name:

print_config: True # save the config in the log folder
mode: {inference, postprocess}
debug: False # will print the complete yaml config plus run a validation test
